# XGBoost

## XGBoost ä»‹ç»
XGBoostï¼ˆeXtreme Gradient Boostingï¼‰æ˜¯æ¢¯åº¦æå‡å†³ç­–æ ‘ï¼ˆGBDTï¼‰çš„é«˜æ•ˆä¼˜åŒ–å®ç°ï¼Œæ ¸å¿ƒæ˜¯é€šè¿‡å‰å‘åˆ†æ­¥åŠ æ³•è®­ç»ƒã€äºŒé˜¶æ³°å‹’å±•å¼€è¿‘ä¼¼æŸå¤±ä¸æ­£åˆ™åŒ–æ§åˆ¶ï¼Œç»“åˆå·¥ç¨‹ä¼˜åŒ–å®ç°é«˜ç²¾åº¦ä¸é«˜æ•ˆç‡ï¼Œå¸¸ç”¨äºåˆ†ç±»ã€å›å½’ã€æ’åºç­‰ä»»åŠ¡ã€‚ä»¥ä¸‹ä»æ ¸å¿ƒåŸç†ã€å…³é”®åˆ›æ–°ã€å·¥ç¨‹ä¼˜åŒ–ä¸‰æ–¹é¢å±•å¼€è¯´æ˜ï¼š

---

### ä¸€ã€æ ¸å¿ƒæ€æƒ³ä¸åŠ æ³•è®­ç»ƒæ¡†æ¶

XGBoostéµå¾ªæ¢¯åº¦æå‡çš„æ ¸å¿ƒé€»è¾‘ï¼šä¸²è¡Œè®­ç»ƒå¤šæ£µå†³ç­–æ ‘ï¼Œæ¯æ£µæ ‘æ‹Ÿåˆå‰ä¸€è½®æ¨¡å‹çš„é¢„æµ‹æ®‹å·®ï¼ˆè´Ÿæ¢¯åº¦ï¼‰ï¼Œæœ€ç»ˆå°†æ‰€æœ‰æ ‘çš„é¢„æµ‹ç»“æœåŠ æƒç´¯åŠ å¾—åˆ°æœ€ç»ˆé¢„æµ‹ï¼Œå…¬å¼å¦‚ä¸‹ï¼š

 $\hat{y}_i^{(t)} = \hat{y}_i^{(t-1)} + f_t(x_i)$ 

å…¶ä¸­ï¼Œ $\hat{y}_i^{(t)}$  æ˜¯ç¬¬  $t$  è½®å¯¹æ ·æœ¬  $i$  çš„é¢„æµ‹å€¼ï¼Œ $f_t$  æ˜¯ç¬¬  $t$  æ£µå†³ç­–æ ‘ï¼Œ $\hat{y}_i^{(t-1)}$  æ˜¯å‰  $t-1$  æ£µæ ‘çš„ç´¯åŠ é¢„æµ‹ç»“æœã€‚è®­ç»ƒç›®æ ‡æ˜¯æœ€å°åŒ–ç›®æ ‡å‡½æ•°ï¼Œé€æ­¥é™ä½æ¨¡å‹è¯¯å·®ã€‚

---

### äºŒã€ç›®æ ‡å‡½æ•°ä¸äºŒé˜¶æ³°å‹’å±•å¼€ï¼ˆæ ¸å¿ƒåˆ›æ–°ï¼‰

XGBoostçš„ç›®æ ‡å‡½æ•°ç”±**è®­ç»ƒæŸå¤±é¡¹**å’Œ**æ­£åˆ™åŒ–é¡¹**ç»„æˆï¼Œèƒ½åœ¨æ‹Ÿåˆæ•°æ®çš„åŒæ—¶æ§åˆ¶æ¨¡å‹å¤æ‚åº¦ï¼Œå…¬å¼å¦‚ä¸‹ï¼š

 $Obj^{(t)} = \sum_{i=1}^n L(y_i, \hat{y}_i^{(t-1)} + f_t(x_i)) + \Omega(f_t) + constant$ 

- **è®­ç»ƒæŸå¤±é¡¹**  $L(y_i, \hat{y}_i^{(t-1)} + f_t(x_i))$ ï¼šè¡¡é‡é¢„æµ‹å€¼ä¸çœŸå®å€¼çš„å·®å¼‚ï¼Œå¦‚å›å½’ç”¨MSEã€åˆ†ç±»ç”¨å¯¹æ•°æŸå¤±ç­‰ã€‚

- **æ­£åˆ™åŒ–é¡¹**  $\Omega(f_t) = \gamma T + \frac{1}{2}\lambda \sum_{j=1}^T w_j^2$ ï¼šæ§åˆ¶æ ‘å¤æ‚åº¦ï¼Œ $\gamma$  æ˜¯å¶èŠ‚ç‚¹æ•°é‡æƒ©ç½šç³»æ•°ï¼Œ $T$  ä¸ºå¶èŠ‚ç‚¹æ•°ï¼Œ $\lambda$  æ˜¯å¶èŠ‚ç‚¹æƒé‡çš„L2æ­£åˆ™ç³»æ•°ï¼Œ $w_j$  æ˜¯ç¬¬  $j$  ä¸ªå¶èŠ‚ç‚¹çš„è¾“å‡ºæƒé‡ã€‚

ä¸ºé«˜æ•ˆä¼˜åŒ–ç›®æ ‡å‡½æ•°ï¼ŒXGBoostå¯¹æŸå¤±é¡¹åœ¨  $\hat{y}_i^{(t-1)}$  å¤„åš**äºŒé˜¶æ³°å‹’å±•å¼€**ï¼Œå¹¶ä¿ç•™å¸¸æ•°é¡¹ï¼Œç›®æ ‡å‡½æ•°å¯ç®€åŒ–ä¸ºï¼š

 $Obj^{(t)} \approx \sum_{i=1}^n [g_i f_t(x_i) + \frac{1}{2} h_i f_t^2(x_i)] + \gamma T + \frac{1}{2}\lambda \sum_{j=1}^T w_j^2$ 

å…¶ä¸­ï¼Œ $g_i = \partial_{\hat{y}_i^{(t-1)}} L(y_i, \hat{y}_i^{(t-1)})$ ï¼ˆä¸€é˜¶æ¢¯åº¦ï¼‰ï¼Œ $h_i = \partial_{\hat{y}_i^{(t-1)}}^2 L(y_i, \hat{y}_i^{(t-1)})$ ï¼ˆäºŒé˜¶æ¢¯åº¦ï¼‰ã€‚è¯¥è¿‘ä¼¼è®©æ¯æ£µæ ‘çš„è®­ç»ƒå¯å¿«é€Ÿæ±‚è§£ï¼Œä¸”ç²¾åº¦é«˜äºä»…ç”¨ä¸€é˜¶æ¢¯åº¦çš„ä¼ ç»ŸGBDTã€‚

è¿›ä¸€æ­¥å‡è®¾æ ‘çš„ç»“æ„å›ºå®šï¼ˆå³æ ·æœ¬åˆ°å¶èŠ‚ç‚¹çš„æ˜ å°„å›ºå®šï¼‰ï¼Œå°†æ ·æœ¬æŒ‰å¶èŠ‚ç‚¹åˆ†ç»„ï¼Œä»¤  $I_j = {i | f_t(x_i) = w_j}$  è¡¨ç¤ºç¬¬  $j$  ä¸ªå¶èŠ‚ç‚¹å¯¹åº”çš„æ ·æœ¬é›†åˆï¼Œç›®æ ‡å‡½æ•°å¯è½¬åŒ–ä¸ºå¶èŠ‚ç‚¹æƒé‡çš„å‡½æ•°ï¼Œæœ€ä¼˜å¶èŠ‚ç‚¹æƒé‡  $w_j^* = -\frac{\sum_{i \in I_j} g_i}{\sum_{i \in I_j} h_i + \lambda}$ ï¼Œä»£å…¥åç›®æ ‡å‡½æ•°çš„æœ€å°å€¼ä¸º  $-\frac{1}{2} \sum_{j=1}^T \frac{(\sum_{i \in I_j} g_i)^2}{\sum_{i \in I_j} h_i + \lambda} + \gamma T$ ã€‚

---

### ä¸‰ã€æ ‘çš„æ„å»ºä¸åˆ†è£‚ç­–ç•¥

æ ‘çš„æ„å»ºé‡‡ç”¨**è´ªå¿ƒç®—æ³•**ï¼Œé€’å½’é€‰æ‹©æœ€ä¼˜åˆ†è£‚ç‚¹ä»¥æœ€å¤§åŒ–ç›®æ ‡å‡½æ•°å¢ç›Šï¼Œæ ¸å¿ƒæ­¥éª¤å¦‚ä¸‹ï¼š

1. åˆå§‹åŒ–å¶èŠ‚ç‚¹ï¼Œè®¡ç®—æ‰€æœ‰æ ·æœ¬çš„  $g_i$  å’Œ  $h_i$  æ€»å’Œã€‚

2. éå†æ¯ä¸ªç‰¹å¾åŠå¯èƒ½çš„åˆ†è£‚ç‚¹ï¼Œè®¡ç®—åˆ†è£‚å‰åçš„ç›®æ ‡å‡½æ•°å·®å€¼ï¼ˆå³å¢ç›Šï¼‰ï¼š

 $Gain = \frac{1}{2} \left[ \frac{(\sum_{L} g_i)^2}{\sum_{L} h_i + \lambda} + \frac{(\sum_{R} g_i)^2}{\sum_{R} h_i + \lambda} - \frac{(\sum_{all} g_i)^2}{\sum_{all} h_i + \lambda} \right] - \gamma$ 

å…¶ä¸­ï¼Œ $\sum_L$ ã€ $\sum_R$  åˆ†åˆ«æ˜¯åˆ†è£‚åå·¦ã€å³å­èŠ‚ç‚¹çš„æ¢¯åº¦å’Œã€‚

1. é€‰æ‹©å¢ç›Šæœ€å¤§çš„åˆ†è£‚ç‚¹è¿›è¡ŒèŠ‚ç‚¹åˆ†è£‚ï¼›è‹¥æœ€å¤§å¢ç›Šå°äº0ï¼Œåˆ™åœæ­¢åˆ†è£‚ï¼ˆå‰ªæï¼‰ã€‚

2. é‡å¤ä¸Šè¿°æ­¥éª¤ï¼Œç›´è‡³è¾¾åˆ°æœ€å¤§æ ‘æ·±ã€æœ€å°æ ·æœ¬æƒé‡å’Œç­‰çº¦æŸæ¡ä»¶ã€‚

æ­¤å¤–ï¼ŒXGBoostæ”¯æŒ**è¿‘ä¼¼åˆ†è£‚ç®—æ³•**ï¼ˆåˆ†æ¡¶æ‰¾å€™é€‰åˆ†è£‚ç‚¹ï¼‰å’Œ**åˆ—é‡‡æ ·**ï¼ˆæ¯è½®è®­ç»ƒä»…ç”¨éƒ¨åˆ†ç‰¹å¾ï¼‰ï¼Œå¹³è¡¡è®­ç»ƒæ•ˆç‡ä¸æ¨¡å‹æ³›åŒ–æ€§ã€‚

---

### å››ã€å…³é”®ç‰¹æ€§ä¸å·¥ç¨‹ä¼˜åŒ–

1. **ç¨€ç–æ„ŸçŸ¥ä¸ç¼ºå¤±å€¼å¤„ç†**ï¼šè®­ç»ƒæ—¶è‡ªåŠ¨å­¦ä¹ ç¼ºå¤±å€¼çš„æœ€ä¼˜åˆ’åˆ†æ–¹å‘ï¼Œé¢„æµ‹æ—¶å°†ç¼ºå¤±å€¼åˆ†é…åˆ°é»˜è®¤æ–¹å‘ï¼Œé€‚é…ç¨€ç–æ•°æ®åœºæ™¯ã€‚

2. **å¹¶è¡ŒåŒ–ä¼˜åŒ–**ï¼šä»¥â€œåˆ—å—â€å­˜å‚¨ç‰¹å¾ï¼Œåœ¨æ ‘èŠ‚ç‚¹åˆ†è£‚æ—¶å¹¶è¡Œè®¡ç®—å„ç‰¹å¾çš„åˆ†è£‚å¢ç›Šï¼Œæå‡è®­ç»ƒé€Ÿåº¦ï¼›æ”¯æŒCPUå¤šçº¿ç¨‹å¹¶è¡Œä¸åˆ†å¸ƒå¼è®­ç»ƒã€‚

3. **ç¼“å­˜ä¼˜åŒ–**ï¼šå°†æ ·æœ¬æ¢¯åº¦ä¸ç‰¹å¾å€¼ç¼“å­˜å¯¹é½ï¼Œå‡å°‘å†…å­˜è®¿é—®å¼€é”€ï¼Œæå‡è®¡ç®—æ•ˆç‡ã€‚

4. **å‰ªæç­–ç•¥**ï¼šé€šè¿‡æ­£åˆ™åŒ–ä¸åˆ†è£‚å¢ç›Šé˜ˆå€¼ï¼ˆ $\gamma$ ï¼‰å®ç°é¢„å‰ªæï¼Œé¿å…è¿‡æ‹Ÿåˆã€‚

---

### äº”ã€é¢„æµ‹ä¸æ¨¡å‹è¾“å‡º

é¢„æµ‹æ—¶ï¼Œè¾“å…¥æ ·æœ¬åœ¨æ¯æ£µæ ‘ä¸­éå†åˆ°å¯¹åº”çš„å¶èŠ‚ç‚¹ï¼Œç´¯åŠ æ‰€æœ‰å¶èŠ‚ç‚¹çš„æƒé‡å€¼ï¼Œç»“åˆå­¦ä¹ ç‡  $\eta$ ï¼ˆæ”¶ç¼©ç³»æ•°ï¼‰å¾—åˆ°æœ€ç»ˆé¢„æµ‹ç»“æœï¼š $\hat{y} = \sum_{t=1}^T \eta f_t(x)$ ï¼Œ $\eta$  ç”¨äºé™ä½å•æ£µæ ‘çš„å½±å“ï¼Œæå‡æ¨¡å‹é²æ£’æ€§ã€‚

---

### æ ¸å¿ƒåŸç†å¯¹æ¯”ï¼ˆXGBoost vs ä¼ ç»ŸGBDTï¼‰

|ç‰¹æ€§|XGBoost|ä¼ ç»ŸGBDT|
|---|---|---|
|æ¢¯åº¦åˆ©ç”¨|ä¸€é˜¶+äºŒé˜¶æ¢¯åº¦ï¼ˆäºŒé˜¶æ³°å‹’å±•å¼€ï¼‰|ä»…ä¸€é˜¶æ¢¯åº¦|
|æ­£åˆ™åŒ–|L1+L2+å¶èŠ‚ç‚¹æ•°æƒ©ç½š|å¼±æ­£åˆ™æˆ–æ— æ˜¾å¼æ­£åˆ™|
|ç¼ºå¤±å€¼å¤„ç†|ç¨€ç–æ„ŸçŸ¥ï¼Œè‡ªåŠ¨å­¦ä¹ åˆ’åˆ†æ–¹å‘|éœ€æ‰‹åŠ¨å¡«å……ç¼ºå¤±å€¼|
|å¹¶è¡Œèƒ½åŠ›|æ”¯æŒç‰¹å¾å¹¶è¡Œã€åˆ†å¸ƒå¼è®­ç»ƒ|ä¸²è¡Œè®­ç»ƒï¼Œæ— å¹¶è¡Œä¼˜åŒ–|
|åˆ†è£‚ç­–ç•¥|ç²¾ç¡®/è¿‘ä¼¼åˆ†è£‚+åˆ—é‡‡æ ·|è´ªå¿ƒç²¾ç¡®åˆ†è£‚|
éœ€è¦æˆ‘åŸºäºä¸Šè¿°åŸç†ï¼Œæä¾›ä¸€ä»½å¯ç›´æ¥è¿è¡Œçš„**XGBooståˆ†ç±»/å›å½’ç¤ºä¾‹ä»£ç **ï¼ˆå«å‚æ•°æ³¨é‡Šä¸ç»“æœè¯„ä¼°ï¼‰å—ï¼Ÿ


## XGBoost çš„å®‰è£…
æ­¤æ¬¡å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬å°†é€šè¿‡jupyter notebook æ¥æ¼”ç¤ºXGBoostçš„ä½¿ç”¨ã€‚æ‰€ä»¥æˆ‘ä»¬éœ€è¦å®‰è£…ä»¥ä¸‹åº“ï¼š
```bash
pip install pandas numpy scikit-learn xgboost matplotlib seaborn jupyter
```
### xgboost
1. ç”¨é€”ï¼šé«˜æ•ˆå®ç°æ¢¯åº¦æå‡å†³ç­–æ ‘ï¼ˆGradient Boostingï¼‰çš„åº“ã€‚
2. ç‰¹ç‚¹ï¼š
    * æ€§èƒ½ä¼˜å¼‚ï¼Œåœ¨å¾ˆå¤š Kaggle æ¯”èµ›ä¸­è¡¨ç°çªå‡ºã€‚
    * æ”¯æŒåˆ†ç±»ã€å›å½’ã€æ’åºç­‰ä»»åŠ¡ã€‚
    * å¯å¤„ç†ç¼ºå¤±å€¼ã€æ”¯æŒå¹¶è¡Œè®¡ç®—ã€æä¾›æ­£åˆ™åŒ–é˜²æ­¢è¿‡æ‹Ÿåˆã€‚
### jupyter
1. ç”¨é€”ï¼šäº¤äº’å¼å¼€å‘ç¯å¢ƒï¼ˆé€šå¸¸æŒ‡ Jupyter Notebook æˆ– JupyterLabï¼‰ã€‚
2. ç‰¹ç‚¹ï¼š
    * å…è®¸åœ¨æµè§ˆå™¨ä¸­ç¼–å†™å’Œè¿è¡Œä»£ç å—ï¼ˆcellï¼‰ï¼Œå³æ—¶æŸ¥çœ‹ç»“æœã€‚
    * æ”¯æŒ Markdown æ–‡æ¡£ã€å…¬å¼ã€å›¾ç‰‡åµŒå…¥ï¼Œéå¸¸é€‚åˆæ•™å­¦ã€æ¢ç´¢æ€§æ•°æ®åˆ†æå’ŒæŠ¥å‘Šæ’°å†™ã€‚
    * å¹¿æ³›ç”¨äºæ•°æ®ç§‘å­¦ã€æœºå™¨å­¦ä¹ ç­‰é¢†åŸŸã€‚
### pandas
1. ç”¨é€”ï¼šç”¨äºæ•°æ®å¤„ç†å’Œåˆ†æã€‚
2. ç‰¹ç‚¹ï¼š
    * æä¾›Â DataFrameÂ å’ŒÂ SeriesÂ æ•°æ®ç»“æ„ï¼Œä¾¿äºæ“ä½œè¡¨æ ¼å‹æˆ–å¼‚æ„å‹æ•°æ®ã€‚
    * æ”¯æŒè¯»å†™ CSVã€Excelã€SQL ç­‰å¤šç§æ ¼å¼ã€‚
    * å¼ºå¤§çš„æ•°æ®æ¸…æ´—ã€ç­›é€‰ã€èšåˆåŠŸèƒ½ã€‚
### numpy
1. ç”¨é€”ï¼šç”¨äºç§‘å­¦è®¡ç®—çš„åŸºç¡€åº“ã€‚
2. ç‰¹ç‚¹ï¼š
    * æä¾›é«˜æ€§èƒ½çš„å¤šç»´æ•°ç»„å¯¹è±¡ï¼ˆndarrayï¼‰ã€‚
    * æ”¯æŒå¹¿æ’­ã€å‘é‡åŒ–è¿ç®—ã€çº¿æ€§ä»£æ•°ã€éšæœºæ•°ç”Ÿæˆç­‰ã€‚
    * æ˜¯ pandasã€scikit-learn ç­‰åº“çš„åº•å±‚ä¾èµ–ã€‚
### scikit-learn
1. ç”¨é€”ï¼šç»å…¸çš„æœºå™¨å­¦ä¹ åº“ã€‚
2. ç‰¹ç‚¹ï¼š
    * æä¾›å¤§é‡ç›‘ç£/æ— ç›‘ç£å­¦ä¹ ç®—æ³•ï¼ˆå¦‚çº¿æ€§å›å½’ã€SVMã€KMeansã€å†³ç­–æ ‘ç­‰ï¼‰ã€‚
    * åŒ…å«æ¨¡å‹è¯„ä¼°ã€äº¤å‰éªŒè¯ã€æ•°æ®é¢„å¤„ç†ã€ç‰¹å¾é€‰æ‹©ç­‰å·¥å…·ã€‚
    * æ¥å£ç»Ÿä¸€ã€æ–‡æ¡£å®Œå–„ï¼Œé€‚åˆåˆå­¦è€…å’Œå·¥ä¸šåº”ç”¨ã€‚
### matplotlib
1. ç”¨é€”ï¼šPython æœ€åŸºç¡€çš„ç»˜å›¾åº“ã€‚
2. ç‰¹ç‚¹ï¼š
    * å¯ç»˜åˆ¶æŠ˜çº¿å›¾ã€æ•£ç‚¹å›¾ã€æŸ±çŠ¶å›¾ã€ç›´æ–¹å›¾ç­‰ã€‚
    * é«˜åº¦å¯å®šåˆ¶ï¼ˆé¢œè‰²ã€æ ‡ç­¾ã€å›¾ä¾‹ã€åæ ‡è½´ç­‰ï¼‰ã€‚
    * æ˜¯è®¸å¤šé«˜çº§å¯è§†åŒ–åº“ï¼ˆå¦‚ seabornï¼‰çš„åº•å±‚ä¾èµ–ã€‚

### seaborn
1. ç”¨é€”ï¼šåŸºäº matplotlib çš„é«˜çº§ç»Ÿè®¡å¯è§†åŒ–åº“ã€‚
2. ç‰¹ç‚¹ï¼š
    * è¯­æ³•æ›´ç®€æ´ï¼Œé€‚åˆå¿«é€Ÿç»˜åˆ¶ç¾è§‚çš„ç»Ÿè®¡å›¾è¡¨ï¼ˆå¦‚çƒ­åŠ›å›¾ã€åˆ†å¸ƒå›¾ã€ç®±çº¿å›¾ç­‰ï¼‰ã€‚
    * å†…ç½®é…è‰²æ–¹æ¡ˆå’Œæ ·å¼ï¼Œå›¾å½¢é»˜è®¤æ›´â€œå¥½çœ‹â€ã€‚
    * ä¸ pandas DataFrame æ— ç¼é›†æˆã€‚

## å­¦ä¹ åœºæ™¯
å…ˆå¯åŠ¨`jupyter notebook`, ç„¶ååˆ›å»ºä¸€ä¸ªæ–°çš„notebookï¼ˆ`file->new->notebook->python 3`ï¼‰
```py
# å¯åŠ¨
jupyter notebook
```
**æ³¨æ„ï¼šæ¯ä¸ªå•å…ƒæ ¼è¾“å…¥å®Œæˆåï¼Œç‚¹å‡»è¿è¡ŒæŒ‰é’®ï¼ˆæˆ–æŒ‰ Shift + Enterï¼‰æ‰§è¡Œä»£ç ã€‚**

### åœºæ™¯1ï¼šæ³°å¦å°¼å…‹å·ç”Ÿå­˜é¢„æµ‹
#### å®Œæ•´ä»£ç 
<details><summary><b>è¯¦ç»†ä»£ç </b></summary>

```py
# 1. å¯¼å…¥åº“
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, classification_report
import xgboost as xgb

# 2. åŠ è½½æ•°æ®ï¼ˆç¡®ä¿ train.csv åœ¨å½“å‰ç›®å½•ï¼‰
df = pd.read_csv('./æ•°æ®/train.csv')

# 3. æŸ¥çœ‹åŸºæœ¬ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰
print("æ•°æ®å½¢çŠ¶:", df.shape)
print("\nç¼ºå¤±å€¼ç»Ÿè®¡:")
print(df.isnull().sum())

# 4. ç‰¹å¾å·¥ç¨‹ä¸é¢„å¤„ç†

# åˆ é™¤æ— å…³åˆ—
df_clean = df.drop(columns=['PassengerId', 'Name', 'Ticket', 'Cabin']).copy()

# å¡«å……ç¼ºå¤±å€¼ï¼ˆä¸å†ä½¿ç”¨ inplace=Trueï¼‰
df_clean['Age'] = df_clean['Age'].fillna(df_clean['Age'].median())
df_clean['Embarked'] = df_clean['Embarked'].fillna(df_clean['Embarked'].mode()[0])

# ç¼–ç åˆ†ç±»å˜é‡
df_clean['Sex'] = df_clean['Sex'].map({'male': 0, 'female': 1})
df_clean['Embarked'] = LabelEncoder().fit_transform(df_clean['Embarked'])

# åˆ›å»ºæ–°ç‰¹å¾
df_clean['FamilySize'] = df_clean['SibSp'] + df_clean['Parch'] + 1
df_clean['IsAlone'] = (df_clean['FamilySize'] == 1).astype(int)

# 5. å‡†å¤‡ç‰¹å¾ X å’Œæ ‡ç­¾ y
X = df_clean[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked', 'FamilySize', 'IsAlone']]
y = df_clean['Survived']

# 6. åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# 7. è®­ç»ƒ XGBoost æ¨¡å‹
model = xgb.XGBClassifier(
    n_estimators=100,
    max_depth=5,
    learning_rate=0.1,
    random_state=42,
    eval_metric='logloss'
)

model.fit(X_train, y_train)

# 8. é¢„æµ‹ä¸è¯„ä¼°
y_pred = model.predict(X_test)

print("\n=== æ¨¡å‹è¯„ä¼° ===")
print("å‡†ç¡®ç‡:", round(accuracy_score(y_test, y_pred), 4))
print("\nåˆ†ç±»æŠ¥å‘Š:")
print(classification_report(y_test, y_pred))
```
</details>

#### å•å…ƒæ ¼ 1ï¼šå¯¼å…¥åº“ï¼ˆç¡®ä¿å·²å®‰è£…ï¼‰
```py
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import xgboost as xgb
```

#### å•å…ƒæ ¼ 2ï¼šåŠ è½½æ•°æ®
```py
# è¯»å–æœ¬åœ° CSV æ–‡ä»¶
df = pd.read_csv('./æ•°æ®/train.csv')  # å‡è®¾æ–‡ä»¶åæ˜¯ titanic.csvï¼Œæ”¾åœ¨å½“å‰ç›®å½•ä¸‹

print("æ•°æ®å½¢çŠ¶:", df.shape)
df.head()
```

#### å•å…ƒæ ¼ 3ï¼šæ•°æ®é¢„å¤„ç†
```py
# åˆ é™¤æ— å…³åˆ—
df_clean = df.drop(columns=['PassengerId', 'Name', 'Ticket', 'Cabin']).copy()

# å¡«å……ç¼ºå¤±å€¼ï¼ˆä¸å†ä½¿ç”¨ inplace=Trueï¼‰
df_clean['Age'] = df_clean['Age'].fillna(df_clean['Age'].median())
df_clean['Embarked'] = df_clean['Embarked'].fillna(df_clean['Embarked'].mode()[0])

# ç¼–ç åˆ†ç±»å˜é‡
df_clean['Sex'] = df_clean['Sex'].map({'male': 0, 'female': 1})
df_clean['Embarked'] = LabelEncoder().fit_transform(df_clean['Embarked'])

# åˆ›å»ºæ–°ç‰¹å¾
df_clean['FamilySize'] = df_clean['SibSp'] + df_clean['Parch'] + 1
df_clean['IsAlone'] = (df_clean['FamilySize'] == 1).astype(int)

# æŸ¥çœ‹ç»“æœ
df_clean.head()
```

#### å•å…ƒæ ¼ 4ï¼šå‡†å¤‡è®­ç»ƒæ•°æ®
```py
X = df_clean[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked', 'FamilySize', 'IsAlone']]
y = df_clean['Survived']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print("è®­ç»ƒé›†å¤§å°:", X_train.shape)
print("æµ‹è¯•é›†å¤§å°:", X_test.shape)
```

#### å•å…ƒæ ¼ 5ï¼šè®­ç»ƒæ¨¡å‹
```py
model = xgb.XGBClassifier(
    n_estimators=100,
    max_depth=5,
    learning_rate=0.1,
    random_state=42,
    eval_metric='logloss'
)

model.fit(X_train, y_train)
```

#### å•å…ƒæ ¼ 6ï¼šè¯„ä¼°ç»“æœ
```py
y_pred = model.predict(X_test)

print("å‡†ç¡®ç‡:", round(accuracy_score(y_test, y_pred), 4))
print("\nåˆ†ç±»æŠ¥å‘Š:")
print(classification_report(y_test, y_pred))
```
æˆåŠŸåä¼šè¾“å‡ºç±»ä¼¼ç»“æœï¼š
```text
å‡†ç¡®ç‡: 0.6257

åˆ†ç±»æŠ¥å‘Š:
              precision    recall  f1-score   support

           0       0.68      0.74      0.71       110
           1       0.52      0.45      0.48        69

    accuracy                           0.63       179
   macro avg       0.60      0.59      0.59       179
weighted avg       0.62      0.63      0.62       179
```

#### ä¿å­˜Notebook
1. ç‚¹å‡»èœå•æ  File â†’ Save and Checkpoint
2. æˆ–æŒ‰å¿«æ·é”® Ctrl + Sï¼ˆWindowsï¼‰ / Cmd + Sï¼ˆMacï¼‰

## å¯¼å‡ºæ¨¡å‹
è®­ç»ƒå¥½çš„æ¨¡å‹å¯ä»¥ç›´æ¥ä¿å­˜ä¸º `.ubj` æ–‡ä»¶ï¼Œåç»­åŠ è½½æ—¶æ— éœ€é‡æ–°è®­ç»ƒã€‚**æ³¨æ„ï¼šä¸è¦ç›´æ¥ä¿å­˜ä¸º `.json` æ–‡ä»¶ï¼Œå› ä¸º XGBoost æ¨¡å‹çš„ `.json` æ–‡ä»¶åŒ…å«äº†æ¨¡å‹çš„å…ƒæ•°æ®ä¸”ä¼šæŠ¥é”™ï¼Œè€Œ `.ubj` æ–‡ä»¶åˆ™æ˜¯äºŒè¿›åˆ¶æ ¼å¼ï¼Œæ›´è½»é‡çº§ã€‚**

å¯¼å‡ºæ¨¡å‹æ—¶ï¼Œéœ€è¦åœ¨ä¹‹å‰è®­ç»ƒæ¨¡å‹çš„ä»£ç åæ·»åŠ ä»¥ä¸‹ä»£ç ï¼šï¼ˆ**å¯ä»¥ç›´æ¥è·Ÿåœ¨è®­ç»ƒæ¨¡å‹çš„ä»£ç åé¢ä¹Ÿå¯ä»¥å•ç‹¬æ”¾åœ¨ä¸€ä¸ªå•å…ƒæ ¼ä¸­ï¼Œå‰ææ˜¯éƒ½éœ€è¦é‡å¤´è¿è¡Œä¸€é**ï¼‰
```py
# ç›´æ¥ä¿å­˜ä¸º .ubjï¼ˆå‡è®¾ model æ˜¯ XGBClassifierï¼‰
booster = model.get_booster()
booster.save_model('titanic_xgboost.ubj')
print("âœ… æ¨¡å‹å·²å¯¼å‡ºä¸º titanic_xgboost.ubj")
```

## ä½¿ç”¨å¯¼å‡ºçš„æ¨¡å‹
å¯¼å‡ºæ¨¡å‹åï¼Œåç»­å¯ä»¥ç›´æ¥åŠ è½½æ¨¡å‹è¿›è¡Œé¢„æµ‹ï¼Œæ— éœ€é‡æ–°è®­ç»ƒã€‚ä»¥ä¸‹æ˜¯åŠ è½½æ¨¡å‹çš„ä»£ç ï¼š
```py
import xgboost as xgb

# åŠ è½½å¯¼å‡ºçš„æ¨¡å‹
booster = xgb.Booster()
booster.load_model('titanic_xgboost.ubj')
print("âœ… æ¨¡å‹å·²åŠ è½½")
```
<details><summary><b>é…å¥—çš„å®Œæ•´ä»£ç å¦‚ä¸‹ï¼š</b></summary>

```py
# predict_from_ubj.py
# åŠŸèƒ½ï¼šåŠ è½½å·²æœ‰çš„ .ubj æ¨¡å‹ â†’ é¢„æµ‹ test.csv â†’ ç”Ÿæˆ submission.csv

import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.preprocessing import LabelEncoder

# ----------------------------
# 1. å®šä¹‰ä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´çš„ç‰¹å¾å·¥ç¨‹å‡½æ•°
# ----------------------------
def preprocess_test(df):
    """
    å¯¹æµ‹è¯•é›†è¿›è¡Œä¸è®­ç»ƒé›†å®Œå…¨ç›¸åŒçš„é¢„å¤„ç†
    """
    df = df.copy()
    
    # åˆ é™¤æ— å…³åˆ—ï¼ˆä¿ç•™ PassengerId ç”¨äºæäº¤ï¼‰
    df = df.drop(columns=['Name', 'Ticket', 'Cabin'], errors='ignore')
    
    # å¡«å……ç¼ºå¤±å€¼ï¼ˆå¿…é¡»ä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼ï¼‰
    df['Age'] = df['Age'].fillna(df['Age'].median())
    df['Fare'] = df['Fare'].fillna(df['Fare'].median())  # test.csv ä¸­ Fare å¯èƒ½ç¼ºå¤±
    df['Embarked'] = df['Embarked'].fillna('S')  # è®­ç»ƒæ—¶ç”¨çš„ä¼—æ•°æ˜¯ 'S'
    
    # ç¼–ç åˆ†ç±»å˜é‡ï¼ˆå¿…é¡»ä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼ï¼‰
    df['Sex'] = df['Sex'].map({'male': 0, 'female': 1})
    
    # Embarked ç¼–ç ï¼šS=0, C=1, Q=2ï¼ˆéœ€ä¸è®­ç»ƒæ—¶é¡ºåºä¸€è‡´ï¼‰
    embarked_map = {'S': 0, 'C': 1, 'Q': 2}
    df['Embarked'] = df['Embarked'].map(embarked_map)
    
    # åˆ›å»ºæ–°ç‰¹å¾ï¼ˆå¿…é¡»ä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼ï¼‰
    df['FamilySize'] = df['SibSp'] + df['Parch'] + 1
    df['IsAlone'] = (df['FamilySize'] == 1).astype(int)
    
    return df

# ----------------------------
# 2. åŠ è½½æµ‹è¯•æ•°æ®
# ----------------------------
print("ğŸ“¥ åŠ è½½æµ‹è¯•æ•°æ®...")
test_df = pd.read_csv('./æ•°æ®/test.csv')
passenger_ids = test_df['PassengerId'].copy()  # ä¿ç•™åŸå§‹ ID

# ----------------------------
# 3. ç‰¹å¾å·¥ç¨‹
# ----------------------------
test_clean = preprocess_test(test_df)

# ç¡®ä¿ç‰¹å¾é¡ºåºä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´ï¼
feature_columns = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked', 'FamilySize', 'IsAlone']
X_test = test_clean[feature_columns]

print(f"âœ… æµ‹è¯•é›†å½¢çŠ¶: {X_test.shape}")

# ----------------------------
# 4. åŠ è½½ .ubj æ¨¡å‹
# ----------------------------
print("ğŸ”„ åŠ è½½æ¨¡å‹ titanic_xgboost.ubj ...")
model = xgb.Booster()
model.load_model('titanic_xgboost.ubj')  # æ”¯æŒ .ubj å’Œ .json

# ----------------------------
# 5. é¢„æµ‹
# ----------------------------
dtest = xgb.DMatrix(X_test)
y_pred_proba = model.predict(dtest)
y_pred = (y_pred_proba > 0.5).astype(int)  # äºŒåˆ†ç±»é˜ˆå€¼

print(f"âœ… é¢„æµ‹å®Œæˆï¼Œæ­£ç±»æ¯”ä¾‹: {y_pred.mean():.2%}")

# ----------------------------
# 6. ç”Ÿæˆæäº¤æ–‡ä»¶
# ----------------------------
submission = pd.DataFrame({
    'PassengerId': passenger_ids,
    'Survived': y_pred
})

submission.to_csv('submission.csv', index=False)
print("ğŸ‰ æäº¤æ–‡ä»¶å·²ä¿å­˜ä¸º submission.csv")

# å¯é€‰ï¼šæ‰“å°å‰å‡ è¡Œ
print("\nğŸ“„ æäº¤æ–‡ä»¶é¢„è§ˆ:")
print(submission.head())
```

</details>